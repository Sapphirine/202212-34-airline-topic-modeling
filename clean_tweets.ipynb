{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3827a1f5-aced-4d2b-9942-d52e136f43c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore, CoherenceModel\n",
    "#import pyldavis\n",
    "\n",
    "import nltk\n",
    "#nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871e8b71-e347-4d8d-829c-1fe9b273c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('airline_tweets_csvs/AmericanAir.csv.gz', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27fe8cbc-ce2b-4f77-80c2-189684ad03f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "37ecaf86-760e-4668-8b1a-a85bd8838521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Twitter Web Client'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['created_at'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ba067cd9-ca8e-4d74-a01c-6ae98386aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bookmarks_twitter_airlines.txt\", \"r\") as myfile:\n",
    "    bookmarks = myfile.read().splitlines()\n",
    "twitter_airline_handles = [bkmk.split(\"(\")[1].split(\")\")[0] for bkmk in bookmarks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683483d-38ca-433b-9cc7-56053ba2f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket airlines into continents/regions\n",
    "\n",
    "region_to_airline_map = {\n",
    "    'Africa': [],\n",
    "    \n",
    "    'United States': [\n",
    "        'SouthwestAir',\n",
    "        'united',\n",
    "        \n",
    "    ],\n",
    "    \n",
    "    'Latin America and Carribean': [\n",
    "        'AviancaNAM',\n",
    "        \n",
    "    ],\n",
    "    \n",
    "    'Central/Eastern Asia': [\n",
    "        'AsianaAirlines', \n",
    "    ],\n",
    "    \n",
    "    'Southern/South-eastern Asia': [\n",
    "        'IndonesiaGaruda', \n",
    "        'CebuPacificAir',\n",
    "        'airasia',\n",
    "        'flyPAL',\n",
    "        \n",
    "    ],\n",
    "    \n",
    "    'Western Asia': [\n",
    "        'etihad',\n",
    "        \n",
    "    ],\n",
    "    \n",
    "    'Europe': [\n",
    "        'KLM',\n",
    "        'British_Airways',\n",
    "        \n",
    "        \n",
    "    ],\n",
    "    \n",
    "    'Australia and New Zealand': []\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b79c3a-5b18-4d96-9d8c-39970f356660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all dataframes into one dataframe\n",
    "\n",
    "saved_csvs_dir = \"airline_tweets_csvs/\"\n",
    "df_columns = [\n",
    "    'tweet_id', \n",
    "    'user',\n",
    "    'created_at', 'text', 'is_reply',\n",
    "    'source', 'lang',\n",
    "    'retweet_count', 'reply_count', 'like_count', 'quote_count',\n",
    "    'detected_domain_entity_pairs',\n",
    "    'annots_found', 'hashtags', 'mentions'\n",
    "]\n",
    "full_df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "for csv_file_path in os.listdir(saved_csvs_dir):\n",
    "    try:\n",
    "        new_df = pd.read_csv(saved_csvs_dir + '/' + csv_file_path, low_memory=False)\n",
    "        full_df = pd.concat([full_df, new_df], axis=0)\n",
    "    except IsADirectoryError:\n",
    "        continue\n",
    "    print(full_df.shape)\n",
    "    # final shape: (59849, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "256d2d6f-c2a8-4068-b653-79ece2c1f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv('all_airline_tweets_english.csv.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2ef8e6e-8645-4c93-8210-617fe68e5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('all_airline_tweets_english.csv.gz', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c9f26d1-ef37-4287-beaf-ca593b773ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>source</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>detected_domain_entity_pairs</th>\n",
       "      <th>annots_found</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1156918651838054400</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-08-01T13:24:35.000Z</td>\n",
       "      <td>RT @delightdoodahs: Cracking @easyJet flight f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...</td>\n",
       "      <td>{'Place_Berlin'}</td>\n",
       "      <td>{'lettheholidaysbegin'}</td>\n",
       "      <td>{'delightdoodahs', 'EDI_Airport', 'easyJet'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1156918495462023168</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-08-01T13:23:58.000Z</td>\n",
       "      <td>RT @vfxflyer: Cloud surfing with @easyjet from...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...</td>\n",
       "      <td>{'Other_BUD', 'Organization_BSL'}</td>\n",
       "      <td>{'lovetofly'}</td>\n",
       "      <td>{'easyJet', 'vfxflyer'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1154809777752629249</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-07-26T17:44:41.000Z</td>\n",
       "      <td>RT @LDNLutonAirport: Trains remain severely di...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Brand_EasyJet', 'Interests and Hobbies_Trave...</td>\n",
       "      <td>{'Other_Thameslink'}</td>\n",
       "      <td>set()</td>\n",
       "      <td>{'LDNLutonAirport'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1154670861376524289</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-07-26T08:32:40.000Z</td>\n",
       "      <td>It may take us a little longer to reply to you...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>{'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1154670731613081600</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-07-26T08:32:09.000Z</td>\n",
       "      <td>Due to adverse weather across the UK, easyJet,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>6.0</td>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>{'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...</td>\n",
       "      <td>{'Place_UK'}</td>\n",
       "      <td>set()</td>\n",
       "      <td>set()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id     user                created_at  \\\n",
       "0  1156918651838054400  easyJet  2019-08-01T13:24:35.000Z   \n",
       "1  1156918495462023168  easyJet  2019-08-01T13:23:58.000Z   \n",
       "2  1154809777752629249  easyJet  2019-07-26T17:44:41.000Z   \n",
       "3  1154670861376524289  easyJet  2019-07-26T08:32:40.000Z   \n",
       "4  1154670731613081600  easyJet  2019-07-26T08:32:09.000Z   \n",
       "\n",
       "                                                text  is_reply    source lang  \\\n",
       "0  RT @delightdoodahs: Cracking @easyJet flight f...       0.0  Sprinklr   en   \n",
       "1  RT @vfxflyer: Cloud surfing with @easyjet from...       0.0  Sprinklr   en   \n",
       "2  RT @LDNLutonAirport: Trains remain severely di...       0.0  Sprinklr   en   \n",
       "3  It may take us a little longer to reply to you...       0.0  Sprinklr   en   \n",
       "4  Due to adverse weather across the UK, easyJet,...       0.0  Sprinklr   en   \n",
       "\n",
       "   retweet_count reply_count like_count quote_count  \\\n",
       "0            3.0           0          0           0   \n",
       "1            1.0           0          0           0   \n",
       "2            2.0           0          0           0   \n",
       "3            2.0          28         11           1   \n",
       "4            6.0          34         31           4   \n",
       "\n",
       "                        detected_domain_entity_pairs  \\\n",
       "0  {'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...   \n",
       "1  {'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...   \n",
       "2  {'Brand_EasyJet', 'Interests and Hobbies_Trave...   \n",
       "3  {'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...   \n",
       "4  {'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...   \n",
       "\n",
       "                        annots_found                 hashtags  \\\n",
       "0                   {'Place_Berlin'}  {'lettheholidaysbegin'}   \n",
       "1  {'Other_BUD', 'Organization_BSL'}            {'lovetofly'}   \n",
       "2               {'Other_Thameslink'}                    set()   \n",
       "3                                NaN                      NaN   \n",
       "4                       {'Place_UK'}                    set()   \n",
       "\n",
       "                                       mentions  \n",
       "0  {'delightdoodahs', 'EDI_Airport', 'easyJet'}  \n",
       "1                       {'easyJet', 'vfxflyer'}  \n",
       "2                           {'LDNLutonAirport'}  \n",
       "3                                           NaN  \n",
       "4                                         set()  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31297cb0-bebc-4232-8f2c-ae816298161f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59849, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc8ac032-dcaf-441a-83b2-d3becd2799f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.dropna(subset=['user', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deedf54f-f66e-4aad-b969-722030bd52cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59848, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4c65328-4e89-4c0f-a57d-1433a3636527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used the following source to discover some common pre-processing steps, \n",
    "# and I created my own functions to clean tweets: \n",
    "# https://pub.towardsai.net/tweet-topic-modeling-part-2-cleaning-and-preprocessing-tweets-e3a08a8b1770\n",
    "\n",
    "\n",
    "def clean_tweet(row):\n",
    "    \"\"\"Performs removal of emojis, link-related text, \n",
    "    retweet-related extra text, and hashtags and mentions.\n",
    "    Further strips punctuation, extra spaces, and \n",
    "    numbers. \n",
    "    \"\"\"\n",
    "    tweet = row['text']\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Removing emojis (not in project scope)\n",
    "        tweet = emoji.replace_emoji(tweet, '')\n",
    "    \n",
    "\n",
    "        # Removing link related text\n",
    "        link_related = [r'http', r'bit.ly/', r'pic.twitter']\n",
    "        for link_to_clean in link_related:\n",
    "            tweet = re.sub(fr'{link_to_clean}\\S+', '', tweet)\n",
    "        tweet = tweet.strip(\"[link]\")\n",
    "\n",
    "        # Removing retweet, mention, or hashtag related text\n",
    "        rt_mtn_hsh_related = [r'RT\\s@', r'@', r'#']\n",
    "        for rt_mtn_hsh_to_clean in rt_mtn_hsh_related:\n",
    "            tweet = re.sub(fr'({rt_mtn_hsh_to_clean}[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "\n",
    "        # Remove punctuation\n",
    "        punctuation = '!”$%&\\’()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
    "        tweet = re.sub('[' + punctuation + ']+', ' ', tweet)\n",
    "\n",
    "        # Remove spaces and numbers\n",
    "        # tweet = re.sub('\\s+', ' ', tweet)\n",
    "        tweet = re.sub('([0-9]+)', '', tweet)\n",
    "        tweet = tweet.strip()\n",
    "        \n",
    "        return tweet\n",
    "        \n",
    "    except TypeError:\n",
    "        print(tweet)\n",
    "        print(row['user'])\n",
    "        print(row['tweet_id'])\n",
    "        return 'CANNOT TRANSFORM'\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "def tokenize_and_lemmatize_tweet(row):\n",
    "    \"\"\"Returns list of tokens from tweet after lemmatizing. \"\"\"\n",
    "    tweet = row['clean_text']\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
    "    \n",
    "    tweet_tokens_lst = []\n",
    "    tokens = gensim.utils.simple_preprocess(tweet)\n",
    "    for token in tokens:\n",
    "        if token not in stopwords and len(token) > 2:\n",
    "            tweet_tokens_lst.append(lemmatizer.lemmatize(token, pos='v'))\n",
    "    \n",
    "    return ' '.join(tweet_tokens_lst)  # tweet\n",
    "\n",
    "\n",
    "def create_lemma_token_lst(row):\n",
    "    tokenized_and_lemmatized_tweet = row['tokenize_lemmatize_text']\n",
    "    return tokenized_and_lemmatized_tweet.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "489c2479-9ab1-4be0-adfb-a14c44b97a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['clean_text'] = full_df.apply(lambda row: clean_tweet(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a706b673-e3f2-486c-944e-b96993aae9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['tokenize_lemmatize_text'] = full_df.apply(lambda row: tokenize_and_lemmatize_tweet(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cafc1b80-a777-496a-ad4f-729c36b359a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['lemma_tokens'] = full_df.apply(lambda row: create_lemma_token_lst(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "737391d3-fb7e-4532-828b-a7b731107ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>source</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>detected_domain_entity_pairs</th>\n",
       "      <th>annots_found</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenize_lemmatize_text</th>\n",
       "      <th>lemma_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1156918651838054400</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-08-01T13:24:35.000Z</td>\n",
       "      <td>RT @delightdoodahs: Cracking @easyJet flight f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...</td>\n",
       "      <td>{'Place_Berlin'}</td>\n",
       "      <td>{'lettheholidaysbegin'}</td>\n",
       "      <td>{'delightdoodahs', 'EDI_Airport', 'easyJet'}</td>\n",
       "      <td>Cracking flight from to Berlin this morning</td>\n",
       "      <td>crack flight berlin morning</td>\n",
       "      <td>[crack, flight, berlin, morning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1156918495462023168</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-08-01T13:23:58.000Z</td>\n",
       "      <td>RT @vfxflyer: Cloud surfing with @easyjet from...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...</td>\n",
       "      <td>{'Other_BUD', 'Organization_BSL'}</td>\n",
       "      <td>{'lovetofly'}</td>\n",
       "      <td>{'easyJet', 'vfxflyer'}</td>\n",
       "      <td>Cloud surfing with from BUD to BSL</td>\n",
       "      <td>cloud surf bud bsl</td>\n",
       "      <td>[cloud, surf, bud, bsl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1154809777752629249</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-07-26T17:44:41.000Z</td>\n",
       "      <td>RT @LDNLutonAirport: Trains remain severely di...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Brand_EasyJet', 'Interests and Hobbies_Trave...</td>\n",
       "      <td>{'Other_Thameslink'}</td>\n",
       "      <td>set()</td>\n",
       "      <td>{'LDNLutonAirport'}</td>\n",
       "      <td>Trains remain severely disrupted on the Thames...</td>\n",
       "      <td>train remain severely disrupt thameslink line ...</td>\n",
       "      <td>[train, remain, severely, disrupt, thameslink,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1154670861376524289</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-07-26T08:32:40.000Z</td>\n",
       "      <td>It may take us a little longer to reply to you...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>{'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It may take us a little longer to reply to you...</td>\n",
       "      <td>little longer reply volume message currently r...</td>\n",
       "      <td>[little, longer, reply, volume, message, curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1154670731613081600</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-07-26T08:32:09.000Z</td>\n",
       "      <td>Due to adverse weather across the UK, easyJet,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>6.0</td>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>{'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...</td>\n",
       "      <td>{'Place_UK'}</td>\n",
       "      <td>set()</td>\n",
       "      <td>set()</td>\n",
       "      <td>Due to adverse weather across the UK easyJet l...</td>\n",
       "      <td>adverse weather easyjet like airlines expect d...</td>\n",
       "      <td>[adverse, weather, easyjet, like, airlines, ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id     user                created_at  \\\n",
       "0  1156918651838054400  easyJet  2019-08-01T13:24:35.000Z   \n",
       "1  1156918495462023168  easyJet  2019-08-01T13:23:58.000Z   \n",
       "2  1154809777752629249  easyJet  2019-07-26T17:44:41.000Z   \n",
       "3  1154670861376524289  easyJet  2019-07-26T08:32:40.000Z   \n",
       "4  1154670731613081600  easyJet  2019-07-26T08:32:09.000Z   \n",
       "\n",
       "                                                text  is_reply    source lang  \\\n",
       "0  RT @delightdoodahs: Cracking @easyJet flight f...       0.0  Sprinklr   en   \n",
       "1  RT @vfxflyer: Cloud surfing with @easyjet from...       0.0  Sprinklr   en   \n",
       "2  RT @LDNLutonAirport: Trains remain severely di...       0.0  Sprinklr   en   \n",
       "3  It may take us a little longer to reply to you...       0.0  Sprinklr   en   \n",
       "4  Due to adverse weather across the UK, easyJet,...       0.0  Sprinklr   en   \n",
       "\n",
       "   retweet_count reply_count like_count quote_count  \\\n",
       "0            3.0           0          0           0   \n",
       "1            1.0           0          0           0   \n",
       "2            2.0           0          0           0   \n",
       "3            2.0          28         11           1   \n",
       "4            6.0          34         31           4   \n",
       "\n",
       "                        detected_domain_entity_pairs  \\\n",
       "0  {'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...   \n",
       "1  {'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...   \n",
       "2  {'Brand_EasyJet', 'Interests and Hobbies_Trave...   \n",
       "3  {'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...   \n",
       "4  {'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...   \n",
       "\n",
       "                        annots_found                 hashtags  \\\n",
       "0                   {'Place_Berlin'}  {'lettheholidaysbegin'}   \n",
       "1  {'Other_BUD', 'Organization_BSL'}            {'lovetofly'}   \n",
       "2               {'Other_Thameslink'}                    set()   \n",
       "3                                NaN                      NaN   \n",
       "4                       {'Place_UK'}                    set()   \n",
       "\n",
       "                                       mentions  \\\n",
       "0  {'delightdoodahs', 'EDI_Airport', 'easyJet'}   \n",
       "1                       {'easyJet', 'vfxflyer'}   \n",
       "2                           {'LDNLutonAirport'}   \n",
       "3                                           NaN   \n",
       "4                                         set()   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0        Cracking flight from to Berlin this morning   \n",
       "1                 Cloud surfing with from BUD to BSL   \n",
       "2  Trains remain severely disrupted on the Thames...   \n",
       "3  It may take us a little longer to reply to you...   \n",
       "4  Due to adverse weather across the UK easyJet l...   \n",
       "\n",
       "                             tokenize_lemmatize_text  \\\n",
       "0                        crack flight berlin morning   \n",
       "1                                 cloud surf bud bsl   \n",
       "2  train remain severely disrupt thameslink line ...   \n",
       "3  little longer reply volume message currently r...   \n",
       "4  adverse weather easyjet like airlines expect d...   \n",
       "\n",
       "                                        lemma_tokens  \n",
       "0                   [crack, flight, berlin, morning]  \n",
       "1                            [cloud, surf, bud, bsl]  \n",
       "2  [train, remain, severely, disrupt, thameslink,...  \n",
       "3  [little, longer, reply, volume, message, curre...  \n",
       "4  [adverse, weather, easyjet, like, airlines, ex...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e153c698-4929-4e61-b18e-bc24ff4b8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of tokens\n",
    "# all_tweets_tokenized_lemmatized = [[twt] for twt in full_df['tokenize_lemmatize_text'].values]\n",
    "# dct = Dictionary(all_tweets_tokenized_lemmatized)\n",
    "dct = Dictionary(full_df['lemma_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d87cb98-3b3f-4a38-a279-6cf7fc4472ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28256\n"
     ]
    }
   ],
   "source": [
    "print(len(dct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "311c1123-70f4-40c4-be1a-2b5f48d4bc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14371\n"
     ]
    }
   ],
   "source": [
    "# Filtering Extremes\n",
    "dct.filter_extremes(no_below=2, no_above=.99)\n",
    "print(len(dct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b802d493-ac96-4b87-b52a-ac3dc1eded22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a corpus object \n",
    "corpus = [dct.doc2bow(d) for d in full_df['lemma_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24fb93aa-2da1-4462-9505-f45c948c61fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59848"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aaa8f300-a0fc-470c-bc41-3ccd022161da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a Base LDA model \n",
    "base_model = LdaMulticore(corpus=corpus, num_topics=5, id2word=dct, workers=12, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f015ab3-8833-4446-8b40-f27e239b2f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for words \n",
    "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in base_model.print_topics()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a049796-0d6b-4689-bf3b-5f0d09591922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Topics\n",
    "topics = [' '.join(t[0:10]) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9079cf5-f8bf-4be0-b062-6cc2db722635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "flight travel book check time passengers airport amp update change\n",
      "\n",
      "------ Topic 1 ------\n",
      "flight new world air year happy airline book travel today\n",
      "\n",
      "------ Topic 2 ------\n",
      "fly flight travel book learn seat swiss visit destination ticket\n",
      "\n",
      "------ Topic 3 ------\n",
      "amp today thank fly flight day new team know work\n",
      "\n",
      "------ Topic 4 ------\n",
      "flight book new day trip fly city start visit today\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the topics\n",
    "for id, t in enumerate(topics): \n",
    "    print(f\"------ Topic {id} ------\")\n",
    "    print(t, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "466496c9-414d-4c7f-82b1-0fd2d0bfc3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.699162251281118\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "# a measure of how good the model is. lower the better\n",
    "base_perplexity = base_model.log_perplexity(corpus)\n",
    "print('\\nPerplexity: ', base_perplexity) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82c6b066-5f27-4493-83c2-62b113e1eb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/big-data-proj/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py:441: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\"stats accumulation interrupted; <= %d documents processed\", self._num_docs)\n",
      "Process AccumulatingWorker-47:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "Process AccumulatingWorker-48:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "Process AccumulatingWorker-49:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/queues.py\", line 195, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/anaconda3/envs/big-data-proj/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [59], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute Coherence Score\u001b[39;00m\n\u001b[1;32m      2\u001b[0m coherence_model \u001b[38;5;241m=\u001b[39m CoherenceModel(model\u001b[38;5;241m=\u001b[39mbase_model, texts\u001b[38;5;241m=\u001b[39mfull_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      3\u001b[0m                                    dictionary\u001b[38;5;241m=\u001b[39mdct, coherence\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_v\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m coherence_lda_model_base \u001b[38;5;241m=\u001b[39m \u001b[43mcoherence_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_coherence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCoherence Score: \u001b[39m\u001b[38;5;124m'\u001b[39m, coherence_lda_model_base)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/big-data-proj/lib/python3.8/site-packages/gensim/models/coherencemodel.py:615\u001b[0m, in \u001b[0;36mCoherenceModel.get_coherence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_coherence\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;124;03m\"\"\"Get coherence value based on pipeline parameters.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m \n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m     confirmed_measures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_coherence_per_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_measures(confirmed_measures)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/big-data-proj/lib/python3.8/site-packages/gensim/models/coherencemodel.py:575\u001b[0m, in \u001b[0;36mCoherenceModel.get_coherence_per_topic\u001b[0;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[1;32m    573\u001b[0m     segmented_topics \u001b[38;5;241m=\u001b[39m measure\u001b[38;5;241m.\u001b[39mseg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopics)\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegmented_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(with_std\u001b[38;5;241m=\u001b[39mwith_std, with_support\u001b[38;5;241m=\u001b[39mwith_support)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoherence \u001b[38;5;129;01min\u001b[39;00m BOOLEAN_DOCUMENT_BASED \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoherence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_w2v\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/big-data-proj/lib/python3.8/site-packages/gensim/models/coherencemodel.py:547\u001b[0m, in \u001b[0;36mCoherenceModel.estimate_probabilities\u001b[0;34m(self, segmented_topics)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoherence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_w2v\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    545\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeyed_vectors\n\u001b[0;32m--> 547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulator\n",
      "File \u001b[0;32m/opt/anaconda3/envs/big-data-proj/lib/python3.8/site-packages/gensim/topic_coherence/probability_estimation.py:156\u001b[0m, in \u001b[0;36mp_boolean_sliding_window\u001b[0;34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[0m\n\u001b[1;32m    154\u001b[0m     accumulator \u001b[38;5;241m=\u001b[39m ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)\n\u001b[1;32m    155\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to estimate probabilities from sliding windows\u001b[39m\u001b[38;5;124m\"\u001b[39m, accumulator)\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maccumulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/big-data-proj/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py:444\u001b[0m, in \u001b[0;36mParallelWordOccurrenceAccumulator.accumulate\u001b[0;34m(self, texts, window_size)\u001b[0m\n\u001b[1;32m    441\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats accumulation interrupted; <= \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m documents processed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_docs)\n\u001b[1;32m    442\u001b[0m     interrupted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m accumulators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterrupted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_accumulators(accumulators)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/big-data-proj/lib/python3.8/site-packages/gensim/topic_coherence/text_analysis.py:521\u001b[0m, in \u001b[0;36mParallelWordOccurrenceAccumulator.terminate_workers\u001b[0;34m(self, input_q, output_q, workers, interrupted)\u001b[0m\n\u001b[1;32m    519\u001b[0m accumulators \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(accumulators) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(workers):\n\u001b[0;32m--> 521\u001b[0m     accumulators\u001b[38;5;241m.\u001b[39mappend(\u001b[43moutput_q\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    522\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m accumulators retrieved from output queue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(accumulators))\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m worker \u001b[38;5;129;01min\u001b[39;00m workers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/queues.py:97\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;129;01mand\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock:\n\u001b[0;32m---> 97\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sem\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative maxlength\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/big-data-proj/lib/python3.8/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=base_model, texts=full_df['lemma_tokens'], \n",
    "                                   dictionary=dct, coherence='c_v')\n",
    "coherence_lda_model_base = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbc9b573-5fe0-4771-8d34-38c60b850739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Using cached pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[18 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If the previous advice does not cover your use case, feel free to report it at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/local/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#!pip3 install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7811d9-5666-463e-8b64-eb3e08a346a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Topic Distance Visualization \n",
    "# pyLDAvis.enable_notebook()\n",
    "# pyLDAvis.gensim.prepare(base_model, corpus, id2word)\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# feed the LDA model into the pyLDAvis instance\n",
    "lda_viz = gensimvis.prepare(base_model, corpus, dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c02c66-d888-4d8a-a8a3-ca84767b9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5fc7585-9983-4431-b3af-3ebfdf4c6244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/big-data-proj/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a42750-08cd-4d86-b382-3278005f08b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccea145-3c53-4451-b235-42d95a366e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7311e-fa25-4483-8ae0-058a708ac9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda2c78d-f822-4290-8a4e-d0bd3f9c18b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee2dd868-526a-4de5-88ca-dacb0a63e9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>source</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>detected_domain_entity_pairs</th>\n",
       "      <th>annots_found</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenize_lemmatize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1156918651838054400</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-08-01T13:24:35.000Z</td>\n",
       "      <td>RT @delightdoodahs: Cracking @easyJet flight f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...</td>\n",
       "      <td>{'Place_Berlin'}</td>\n",
       "      <td>{'lettheholidaysbegin'}</td>\n",
       "      <td>{'delightdoodahs', 'EDI_Airport', 'easyJet'}</td>\n",
       "      <td>Cracking flight from to Berlin this morning</td>\n",
       "      <td>crack flight berlin morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1156918495462023168</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-08-01T13:23:58.000Z</td>\n",
       "      <td>RT @vfxflyer: Cloud surfing with @easyjet from...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...</td>\n",
       "      <td>{'Other_BUD', 'Organization_BSL'}</td>\n",
       "      <td>{'lovetofly'}</td>\n",
       "      <td>{'easyJet', 'vfxflyer'}</td>\n",
       "      <td>Cloud surfing with from BUD to BSL</td>\n",
       "      <td>cloud surf bud bsl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1154809777752629249</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-07-26T17:44:41.000Z</td>\n",
       "      <td>RT @LDNLutonAirport: Trains remain severely di...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Brand_EasyJet', 'Interests and Hobbies_Trave...</td>\n",
       "      <td>{'Other_Thameslink'}</td>\n",
       "      <td>set()</td>\n",
       "      <td>{'LDNLutonAirport'}</td>\n",
       "      <td>Trains remain severely disrupted on the Thames...</td>\n",
       "      <td>train remain severely disrupt thameslink line ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1154670861376524289</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-07-26T08:32:40.000Z</td>\n",
       "      <td>It may take us a little longer to reply to you...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>{'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It may take us a little longer to reply to you...</td>\n",
       "      <td>little longer reply volume message currently r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1154670731613081600</td>\n",
       "      <td>easyJet</td>\n",
       "      <td>2019-07-26T08:32:09.000Z</td>\n",
       "      <td>Due to adverse weather across the UK, easyJet,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sprinklr</td>\n",
       "      <td>en</td>\n",
       "      <td>6.0</td>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>{'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...</td>\n",
       "      <td>{'Place_UK'}</td>\n",
       "      <td>set()</td>\n",
       "      <td>set()</td>\n",
       "      <td>Due to adverse weather across the UK easyJet l...</td>\n",
       "      <td>adverse weather easyjet like airlines expect d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id     user                created_at  \\\n",
       "0  1156918651838054400  easyJet  2019-08-01T13:24:35.000Z   \n",
       "1  1156918495462023168  easyJet  2019-08-01T13:23:58.000Z   \n",
       "2  1154809777752629249  easyJet  2019-07-26T17:44:41.000Z   \n",
       "3  1154670861376524289  easyJet  2019-07-26T08:32:40.000Z   \n",
       "4  1154670731613081600  easyJet  2019-07-26T08:32:09.000Z   \n",
       "\n",
       "                                                text  is_reply    source lang  \\\n",
       "0  RT @delightdoodahs: Cracking @easyJet flight f...       0.0  Sprinklr   en   \n",
       "1  RT @vfxflyer: Cloud surfing with @easyjet from...       0.0  Sprinklr   en   \n",
       "2  RT @LDNLutonAirport: Trains remain severely di...       0.0  Sprinklr   en   \n",
       "3  It may take us a little longer to reply to you...       0.0  Sprinklr   en   \n",
       "4  Due to adverse weather across the UK, easyJet,...       0.0  Sprinklr   en   \n",
       "\n",
       "   retweet_count reply_count like_count quote_count  \\\n",
       "0            3.0           0          0           0   \n",
       "1            1.0           0          0           0   \n",
       "2            2.0           0          0           0   \n",
       "3            2.0          28         11           1   \n",
       "4            6.0          34         31           4   \n",
       "\n",
       "                        detected_domain_entity_pairs  \\\n",
       "0  {'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...   \n",
       "1  {'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...   \n",
       "2  {'Brand_EasyJet', 'Interests and Hobbies_Trave...   \n",
       "3  {'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...   \n",
       "4  {'Brand_EasyJet', 'Unified Twitter Taxonomy_Fa...   \n",
       "\n",
       "                        annots_found                 hashtags  \\\n",
       "0                   {'Place_Berlin'}  {'lettheholidaysbegin'}   \n",
       "1  {'Other_BUD', 'Organization_BSL'}            {'lovetofly'}   \n",
       "2               {'Other_Thameslink'}                    set()   \n",
       "3                                NaN                      NaN   \n",
       "4                       {'Place_UK'}                    set()   \n",
       "\n",
       "                                       mentions  \\\n",
       "0  {'delightdoodahs', 'EDI_Airport', 'easyJet'}   \n",
       "1                       {'easyJet', 'vfxflyer'}   \n",
       "2                           {'LDNLutonAirport'}   \n",
       "3                                           NaN   \n",
       "4                                         set()   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0        Cracking flight from to Berlin this morning   \n",
       "1                 Cloud surfing with from BUD to BSL   \n",
       "2  Trains remain severely disrupted on the Thames...   \n",
       "3  It may take us a little longer to reply to you...   \n",
       "4  Due to adverse weather across the UK easyJet l...   \n",
       "\n",
       "                             tokenize_lemmatize_text  \n",
       "0                        crack flight berlin morning  \n",
       "1                                 cloud surf bud bsl  \n",
       "2  train remain severely disrupt thameslink line ...  \n",
       "3  little longer reply volume message currently r...  \n",
       "4  adverse weather easyjet like airlines expect d...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c39fa-dfe9-44c8-a554-3b300f7a30bb",
   "metadata": {},
   "source": [
    "### Appendix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940cab3e-ea40-431a-a3e3-d9200fc5937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    return emoji.get_emoji_regexp().sub(r'', text.decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f4b1d2f-54a3-4c97-9f4e-745d7a6b13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweet = 'We hope everyone is enjoying their holiday season — we know this couple is! 🎁💍\\nhttps://t.co/EzAVQp4LZj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20187c4a-1a32-4428-b1e3-d8cac903acf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We hope everyone is enjoying their holiday season — we know this couple is! \\nhttps://t.co/EzAVQp4LZj'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.replace_emoji(example_tweet, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "187742c2-cbbe-43d1-98e5-aecee456e72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @delightdoodahs: Cracking @easyJet flight from @EDI_Airport to Berlin this morning #lettheholidaysbegin https://t.co/rJJNXu9RO7'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.iloc[0, :]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7049b580-5487-494c-aa14-9aabf74b3efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @delightdoodahs: Cracking @easyJet flight from @EDI_Airport to Berlin this morning #lettheholidaysbegin https://t.co/rJJNXu9RO7'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = full_df.iloc[0, :]['text']\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e91ba4a-9a2c-49e1-9735-6cde078dbcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @delightdoodahs: Cracking @easyJet flight from @EDI_Airport to Berlin this morning #lettheholidaysbegin https://t.co/rJJNXu9RO7'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing emojis (not in project scope)\n",
    "tweet = emoji.replace_emoji(tweet, '')\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c08b5840-0149-430c-a363-1b71b684a5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @delightdoodahs: Cracking @easyJet flight from @EDI_Airport to Berlin this morning #lettheholidaysbegin '"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing link related text\n",
    "link_related = [r'http', r'bit.ly/', r'pic.twitter']\n",
    "for link_to_clean in link_related:\n",
    "    tweet = re.sub(fr'{link_to_clean}\\S+', '', tweet)\n",
    "tweet = tweet.strip(\"[link]\")\n",
    "\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b0c90a8-5a47-4c0b-bf2a-44f8649f2e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': Cracking  flight from  to Berlin this morning  '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing retweet, mention, or hashtag related text\n",
    "rt_mtn_hsh_related = [r'RT\\s@', r'@', r'#']\n",
    "for rt_mtn_hsh_to_clean in rt_mtn_hsh_related:\n",
    "    tweet = re.sub(fr'({rt_mtn_hsh_to_clean}[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)\n",
    "    \n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d50474bc-0fe3-4105-a3c7-f95e3651a2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Cracking  flight from  to Berlin this morning  '"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "punctuation = '!”$%&\\’()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
    "tweet = re.sub('[' + punctuation + ']+', ' ', tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5e2d488-8ae5-460d-9179-92c35223e8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cracking flight from to Berlin this morning'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove spaces and numbers\n",
    "tweet = re.sub('\\s+', ' ', tweet)\n",
    "tweet = re.sub('([0-9]+)', '', tweet)\n",
    "tweet = tweet.strip()\n",
    "\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec057eb4-ec97-410d-a27a-b8027ef89944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c38829-91fa-4ed2-ac7f-1fa311c4c029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39022efb-9f9b-4cfa-bf1e-1e762c0abebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http\\\\S+'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_str = r'http'\n",
    "fr'{example_str}\\S+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf27fe85-bca0-409d-b3f1-0bc2389156ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http\\\\S+'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r'http\\S+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a0b1f-2b18-4d3e-acf9-02a3f5426728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://pub.towardsai.net/tweet-topic-modeling-part-2-cleaning-and-preprocessing-tweets-e3a08a8b1770\n",
    "\n",
    "def remove_links(tweet):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "def remove_hashtags(tweet):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hash tags\n",
    "    return tweet\n",
    "\n",
    "def remove_av(tweet):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n",
    "    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n",
    "    return tweet\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Returns tokenized representation of words in lemma form excluding stopwords\"\"\"\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(tweet):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS \\\n",
    "                and len(token) > 2:  # drops words with less than 3 characters\n",
    "            result.append(lemmatize(token))\n",
    "    return result\n",
    "\n",
    "def lemmatize(token):\n",
    "    \"\"\"Returns lemmatization of a token\"\"\"\n",
    "    return WordNetLemmatizer().lemmatize(token, pos='v')\n",
    "\n",
    "def preprocess_tweet(tweet, lem_and_token=False):\n",
    "    \"\"\"Main master function to clean tweets, stripping noisy characters, and tokenizing use lemmatization\"\"\"\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = tweet.lower()  # lower case\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
    "    if lem_and_token:\n",
    "        tweet_token_list = tokenize(tweet)  # apply lemmatization and tokenization\n",
    "        tweet = ' '.join(tweet_token_list)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0cfbd6-cca0-4600-b2e6-bff6cc0ebfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86f290-05c9-4f4d-ad61-2ab0c157c9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5e909-2992-4d9a-a6e8-9ee92340e524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0091d31c-21c1-416c-9cdb-402fd04c04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csv = \"airline_tweets_csvs/AerLingus.csv.gz\"\n",
    "df = pd.read_csv(path_csv, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ca620-d8fd-48c9-b160-1d25813dd002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (big-data-proj)",
   "language": "python",
   "name": "big-data-proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
